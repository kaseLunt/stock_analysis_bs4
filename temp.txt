stock_analysis_etl/
├── data/
│   └── raw/
│   └── processed/
├── db/
│   └── stock_data.db
├── notebooks/
│   └── exploratory_analysis.ipynb
├── src/
│   ├── etl/
│   │   ├── __init__.py
│   │   ├── data_extraction.py
│   │   ├── data_transformation.py
│   │   └── data_loading.py
│   ├── utils/
│   │   ├── __init__.py
│   │   └── helpers.py
│   └── config.py
├── tests/
│   ├── test_data_extraction.py
│   ├── test_data_transformation.py
│   └── test_data_loading.py
├── requirements.txt
└── run_etl_analysis.py

Stock Analysis ETL Project Roadmap
Phase 1: Data Extraction
Task 1.1: Research and identify data sources (Google Finance, other news sites).
Milestone: Final list of data sources.
Task 1.2: Develop web scraping functions for Google Finance.
Milestone: Function to fetch stock prices, news, etc.
Task 1.3: Develop web scraping functions for additional data sources (if applicable).
Milestone: Additional scraping functions.
Task 1.4: Implement error handling and rate limiting.
Milestone: Robust scraping functions.
Phase 2: Data Transformation
Task 2.1: Implement data cleaning functions.
Milestone: Cleaned data.
Task 2.2: Develop feature engineering functions (e.g., calculating moving averages).
Milestone: Engineered features.
Task 2.3: Implement normalization and data type conversion.
Milestone: Normalized data ready for analysis.
Phase 3: Data Loading
Task 3.1: Set up the database schema.
Milestone: Database schema ready.
Task 3.2: Develop functions to insert data into the database.
Milestone: Data insertion functions.
Task 3.3: Implement batch and real-time data loading.
Milestone: Both batch and real-time data loading capabilities.
Phase 4: Analysis and Visualization
Task 4.1: Develop statistical analysis functions.
Milestone: Basic statistical analyses.
Task 4.2: Implement machine learning models (if applicable).
Milestone: Trained models.
Task 4.3: Create visualizations.
Milestone: Interactive dashboards or plots.
Phase 5: Testing and Deployment
Task 5.1: Write unit tests for all ETL processes.
Milestone: Comprehensive test coverage.
Task 5.2: Conduct performance testing.
Milestone: Optimized code.
Task 5.3: Deploy the project.
Milestone: Live project.
Phase 6: Documentation and Cleanup
Task 6.1: Update README and documentation.
Milestone: Completed documentation.
Task 6.2: Code review and refactoring.
Milestone: Clean and maintainable codebase.

data_extraction.py
# Import necessary libraries
import requests
import pandas as pd
from bs4 import BeautifulSoup
import logging
import os

# Initialize logging for debugging and tracking
logging.basicConfig(level=logging.INFO)

# Constants for API URLs and headers
GOOGLE_FINANCE_URL = 'https://www.google.com/finance/quote/{symbol}:{exchange}?hl=en'
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Get the directory of the currently executing script
dir_path = os.path.dirname(os.path.realpath(__file__))
csv_file_path = os.path.join(dir_path, '../../data/raw/nasdaq_stocks.csv')

# Function to make HTTP requests and handle exceptions
def make_request(url):
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        return response.content
    except requests.HTTPError as e:
        logging.error(f"Failed to retrieve {url}, status code: {response.status_code}, error: {e}")
        return None

# Function to fetch stock prices from multiple exchanges
def fetch_stock_prices(symbol, exchanges=['NASDAQ', 'NYSE']):
    stock_price = None  # Initialize stock price as None
    for exchange in exchanges:
        url = GOOGLE_FINANCE_URL.format(symbol=symbol, exchange=exchange)
        content = make_request(url)
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            stock_price_element = soup.find('div', {'class': 'YMlKec fxKbKc'})
            if stock_price_element:
                stock_price = stock_price_element.text
                break  # Exit the loop if a stock price is found
    return stock_price

# Function to fetch financial metrics from multiple exchanges
def fetch_financial_metrics(symbol, exchanges=['NASDAQ', 'NYSE']):
    financial_metrics = None  # Initialize financial metrics as None
    for exchange in exchanges:
        url = GOOGLE_FINANCE_URL.format(symbol=symbol, exchange=exchange)
        content = make_request(url)
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            metrics_table = soup.find('table', {'class': 'slpEwd'})
            if metrics_table:
                financial_metrics = {}  # Initialize an empty dictionary for financial metrics
                rows = metrics_table.find_all('tr', {'class': 'roXhBd'})
                for row in rows[1:]:  # Skip the header row
                    metric_td = row.find('td', {'class': 'J9Jhg'})
                    metric_name = metric_td.find('div').text.strip()
                    metric_value = row.find('td', {'class': 'QXDnM'}).text.strip()
                    financial_metrics[metric_name] = metric_value
                break  # Exit loop if metrics are found
    return financial_metrics

# Main function to fetch all stock data for NASDAQ and NYSE stocks
def fetch_all_data_for_stocks():
    df = pd.read_csv(csv_file_path)
    df['Market Cap'] = df['Market Cap'].apply(pd.to_numeric, errors='coerce')
    df = df.nlargest(20, 'Market Cap')  # Filter top 20 stocks by Market Cap
    stock_data = []  # Initialize an empty list for stock data
    for index, row in df.iterrows():
        symbol = row['Symbol']
        name = row['Name']
        logging.info(f"Fetching data for {name} ({symbol})")
        stock_price = fetch_stock_prices(symbol)
        financial_metrics = fetch_financial_metrics(symbol)
        stock_data.append({
            'Symbol': symbol,
            'Name': name,
            'Stock_Price': stock_price,
            'Financial_Metrics': financial_metrics
        })
    stock_data_df = pd.DataFrame(stock_data)
    stock_data_df.to_csv('stock_data.csv', index=False)  # Save data to CSV

# Entry point of the script
if __name__ == "__main__":
    fetch_all_data_for_stocks()


data_transformation.py
import pandas as pd
from sklearn.preprocessing import MinMaxScaler


# Function to handle missing values by filling NaNs with zeros
def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:
    """Replace NaN values with zeros.
    
    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        
    Returns:
        pd.DataFrame: DataFrame with NaN values replaced.
    """
    df.fillna(0, inplace=True)
    return df

# Function to convert data types of columns
def type_conversion(df: pd.DataFrame) -> pd.DataFrame:
    """Convert the data types of specific columns.
    
    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        
    Returns:
        pd.DataFrame: DataFrame with converted data types.
    """
    df['Revenue'] = df['Revenue'].astype(float)
    return df

# Function to clean string columns
def clean_strings(df: pd.DataFrame) -> pd.DataFrame:
    """Clean the strings in the DataFrame.
    
    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        
    Returns:
        pd.DataFrame: DataFrame with cleaned strings.
    """
    df['Company'] = df['Company'].str.strip().str.upper()
    return df

# Function to handle outliers based on IQR
def handle_outliers(df: pd.DataFrame, column: str) -> pd.DataFrame:
    """Identify and remove outliers in a given column.
    
    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        column (str): The column to check for outliers.
        
    Returns:
        pd.DataFrame: DataFrame with outliers removed.
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    df = df[(df[column] >= (Q1 - 1.5 * IQR)) & (df[column] <= (Q3 + 1.5 * IQR))]
    return df

# Function to normalize a column
def normalize(df: pd.DataFrame, column: str) -> pd.DataFrame:
    """Normalize a specific column using MinMax scaling.
    
    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        column (str): The column to normalize.
        
    Returns:
        pd.DataFrame: DataFrame with normalized column.
    """
    scaler = MinMaxScaler()
    df[column] = scaler.fit_transform(df[[column]])
    return df

# Main function to clean data
def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """Perform all cleaning operations on the DataFrame.
    
    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        
    Returns:
        pd.DataFrame: Cleaned DataFrame.
    """
    df = handle_missing_values(df)
    df = type_conversion(df)
    df = clean_strings(df)
    df = handle_outliers(df, 'Revenue')
    df = normalize(df, 'Revenue')
    return df

if __name__ == "__main__":
    # Read the stock_data.csv generated by data_extraction.py
    stock_data_df = pd.read_csv('stock_data.csv')
    # Apply transformations
    cleaned_stock_data_df = clean_data(stock_data_df)
    print(cleaned_stock_data_df)

